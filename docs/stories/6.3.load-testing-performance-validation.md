<!-- Powered by BMAD™ Core -->

# Story 6.3: Load Testing & Performance Validation

## Status
Ready for Implementation

**Date de début:** TBD
**Date de completion:** TBD

## Story
**As a** developer,
**I want** to validate the system handles 100 concurrent users,
**so that** I can scale to 50+ paying customers confidently (per NFR6).

## Dependencies
- **Story 1.2** (LinkedIn Profile Scraping) - N8N workflow to test
- **Story 1.5** (Email Campaign Infrastructure) - Email queue to test
- **Story 1.3** (AI-Powered Contextual Enrichment) - AI enrichment to test
- **Story 6.1** (Application Monitoring & Alerting) - Monitoring to track performance

## Acceptance Criteria
1. Load testing tool setup (k6 or Artillery) with test scenarios
2. Scenario 1: 100 concurrent users scraping LinkedIn simultaneously (validate N8N capacity)
3. Scenario 2: 1000 email queue operations (validate Upstash Redis performance)
4. Scenario 3: 100 AI enrichment requests (validate Claude API rate limit handling)
5. Performance benchmarks: API response time <500ms p95, workflow execution <5 seconds (per NFR3)
6. Database query optimization: Add indexes on frequently queried fields (user_id, status, created_at)
7. Report: Document bottlenecks, scaling limits, recommended infrastructure upgrades for 100+ users

## Tasks / Subtasks

- [ ] **Task 1: Set up load testing tool** (AC: 1)
  - [ ] Evaluate load testing tools: k6 vs Artillery
    - **k6:** JavaScript-based, good for API testing, free/open-source
    - **Artillery:** YAML-based, good for API and web testing, free/open-source
    - **Decision:** Choose k6 (better for API-focused testing, easier to integrate with CI/CD)
  - [ ] Install k6:
    - macOS: `brew install k6`
    - Linux: Follow k6 installation guide: https://k6.io/docs/getting-started/installation/
    - Windows: Use WSL or Docker: `docker run --rm -i grafana/k6 run -`
    - Verify installation: `k6 version`
    - **Minimum version:** k6 v0.47.0+ (verify compatibility)
  - [ ] Create load testing directory: `tests/load/`
  - [ ] Create k6 test structure:
    - Base test file: `tests/load/base.js` (common functions, helpers)
    - Test scenarios: `tests/load/scenarios/*.js`
    - Test data: `tests/load/data/*.json`
    - Results directory: `tests/load/results/` (create if not exists)
    - **Note:** Add `tests/load/results/` to `.gitignore` (results files can be large)
  - [ ] Configure k6 test environment:
    - Environment variables: `API_BASE_URL`, `API_KEY`, `N8N_WEBHOOK_URL`
    - **Test users: Create test user accounts in staging environment**
      - [ ] Create test user accounts (10-20 users for load testing)
      - [ ] Store test user credentials in `tests/load/data/test-users.json` (gitignored)
      - [ ] **Test user naming:** `loadtest-user-{1..20}@test.sales-machine.com`
      - [ ] **Test user cleanup:** Document cleanup procedure after tests
      - [ ] **Test user isolation:** Ensure test users don't interfere with each other
      - [ ] **Test data cleanup:** Document procedure to clean test data after load tests
    - Test data: Generate test prospects, campaigns, etc.
  - [ ] **Set up staging environment (if not exists):**
    - [ ] Verify staging environment exists:
      - Staging API URL: `https://api-staging.sales-machine.com` (or equivalent)
      - Staging Supabase project (separate from production)
      - Staging N8N workspace (or use production with test workflows)
    - [ ] If staging doesn't exist:
      - Create staging Supabase project
      - Deploy staging API instance
      - Configure staging environment variables
      - Run database migrations on staging
    - [ ] Document staging environment URLs and credentials
    - [ ] **Note:** Load tests should NEVER run against production
  - [ ] Create k6 test script template:
    - Import HTTP module
    - Define test options (vus, duration, thresholds)
    - Define test scenarios
    - Define setup/teardown functions
  - [ ] Document k6 setup: `tests/load/README.md`
    - Installation instructions
    - How to run tests
    - How to interpret results

- [ ] **Task 2: Create Scenario 1 - 100 concurrent LinkedIn scraping users** (AC: 2)
  - [ ] Create k6 test script: `tests/load/scenarios/linkedin-scraping.js`
  - [ ] **Verify N8N webhook endpoint:**
    - [ ] Check N8N workflow: `workflows/linkedin-scraper.json` (or equivalent)
    - [ ] Verify webhook trigger configuration
    - [ ] Document actual webhook URL pattern: `https://n8n.srv997159.hstgr.cloud/webhook/{workflow-name}/{unique-id}`
    - [ ] Verify webhook authentication method (API key, JWT, or none)
    - [ ] Test webhook manually: `curl -X POST {webhook-url} -d '{...}'`
    - [ ] Update test script with actual webhook URL and authentication
  - [ ] Define test scenario:
    - Virtual users: 100
    - Duration: 10 minutes (ramp-up: 2 minutes, steady: 6 minutes, ramp-down: 2 minutes)
    - Test flow:
      1. Authenticate user (get JWT token)
      2. Trigger LinkedIn scraping workflow via N8N webhook
      3. Wait for workflow completion (poll status)
      4. Verify prospect created in database
  - [ ] Configure N8N webhook endpoint:
    - Webhook URL: `POST /webhooks/n8n/linkedin-scrape`
    - Payload: `{ user_id, search_criteria, ... }`
    - Authentication: API key or JWT token
  - [ ] Add performance thresholds:
    - HTTP request duration: p95 < 5s (workflow trigger)
    - Workflow execution time: p95 < 30s (LinkedIn scraping)
    - Error rate: < 1%
  - [ ] Monitor N8N during test:
    - Track workflow execution queue
    - Track workflow failures
    - Track N8N resource usage (CPU, memory)
  - [ ] Monitor Supabase during test:
    - Track database connections
    - Track query performance
    - Track database CPU/memory usage
  - [ ] Run test and collect results:
    - k6 output: `tests/load/results/linkedin-scraping-*.json`
    - N8N metrics: Screenshots, execution logs
    - Supabase metrics: Query performance, connection pool usage
  - [ ] Analyze results:
    - Identify bottlenecks (N8N queue, database, API)
    - Document failure points
    - Document scaling limits

- [ ] **Task 3: Create Scenario 2 - 1000 email queue operations** (AC: 3)
  - [ ] Create k6 test script: `tests/load/scenarios/email-queue.js`
  - [ ] Define test scenario:
    - Virtual users: 50 (each user performs 20 operations)
    - Total operations: 1000
    - Duration: 5 minutes
    - Test flow:
      1. Authenticate user
      2. Create email campaign
      3. Add prospects to campaign (batch: 10 prospects)
      4. Trigger email send (add to queue)
      5. Verify email queued in Upstash Redis
      6. Poll queue status
  - [ ] Configure Upstash Redis operations:
    - Queue operations: `LPUSH`, `RPOP`, `LLEN` (queue length)
    - Cache operations: `SET`, `GET` (enrichment cache)
  - [ ] Add performance thresholds:
    - Redis operation duration: p95 < 100ms
    - Queue operation duration: p95 < 200ms
    - Error rate: < 0.5%
  - [ ] Monitor Upstash during test:
    - Track Redis commands/second
    - Track Redis memory usage
    - Track Redis latency
  - [ ] Monitor API during test:
    - Track API response times
    - Track API error rate
    - Track API resource usage
  - [ ] Run test and collect results:
    - k6 output: `tests/load/results/email-queue-*.json`
    - Upstash metrics: Dashboard screenshots
    - API metrics: Response times, error rates
  - [ ] Analyze results:
    - Identify bottlenecks (Redis, API, database)
    - Document Redis performance limits
    - Document queue processing capacity

- [ ] **Task 4: Create Scenario 3 - 100 AI enrichment requests** (AC: 4)
  - [ ] Create k6 test script: `tests/load/scenarios/ai-enrichment.js`
  - [ ] **Verify enrichment endpoint:**
    - [ ] Check if endpoint exists: `POST /prospects/:id/enrichment`
    - [ ] **Alternative endpoints to check:**
      - `POST /prospects/:id/enrich` (if different naming)
      - `POST /enrichment` (if different structure)
      - N8N webhook trigger (if enrichment is workflow-based, not API endpoint)
    - [ ] Document actual endpoint or workflow trigger
    - [ ] Update test script with correct endpoint/method
  - [ ] Define test scenario:
    - Virtual users: 100
    - Duration: 5 minutes (ramp-up: 1 minute, steady: 3 minutes, ramp-down: 1 minute)
    - Test flow:
      1. Authenticate user
      2. Create prospect
      3. Trigger AI enrichment via API: `POST /prospects/:id/enrichment` (verify actual endpoint above)
      4. Wait for enrichment completion (poll status)
      5. Verify enrichment data stored
  - [ ] Configure Claude API rate limits:
    - Check Claude API rate limits: Requests/minute, tokens/minute
    - **Document actual rate limits:**
      - Check Claude API documentation: https://docs.anthropic.com/claude/reference/rate-limits
      - Document rate limits for your API tier (free, paid, etc.)
      - Typical limits: 50 requests/minute (free tier), 1000 requests/minute (paid tier)
      - Token limits: Varies by model (Claude 3 Opus, Sonnet, Haiku)
    - Implement rate limiting in test (if needed)
    - Monitor rate limit errors (429 status codes)
    - **Note:** Rate limits may vary by API key tier - verify your specific limits
  - [ ] Add performance thresholds:
    - API response time: p95 < 2s (enrichment trigger)
    - Enrichment completion: p95 < 10s (Claude API + processing)
    - Error rate: < 2% (account for rate limits)
  - [ ] Monitor Claude API during test:
    - Track API requests/minute
    - Track rate limit errors
    - Track token usage
  - [ ] Monitor API Gateway during test:
    - Track enrichment endpoint performance
    - Track error rates
    - Track resource usage
  - [ ] Run test and collect results:
    - k6 output: `tests/load/results/ai-enrichment-*.json`
    - Claude API metrics: Request logs, rate limit errors
    - API metrics: Response times, error rates
  - [ ] Analyze results:
    - Identify bottlenecks (Claude API, API Gateway, database)
    - Document rate limit handling
    - Document enrichment processing capacity

- [ ] **Task 5: Validate performance benchmarks** (AC: 5)
  - [ ] Create performance test script: `tests/load/scenarios/performance-benchmarks.js`
  - [ ] Define benchmark tests:
    - API endpoints: `GET /prospects`, `POST /prospects`, `GET /campaigns`, etc.
    - Virtual users: 50
    - Duration: 5 minutes
    - Test all critical API endpoints
  - [ ] Add performance thresholds (per NFR3):
    - API response time: p95 < 500ms
    - Workflow execution: p95 < 5 seconds
    - Error rate: < 1%
  - [ ] Run benchmark tests:
    - Execute all scenarios
    - Collect metrics: p50, p95, p99 response times
    - Collect error rates
  - [ ] Compare results to NFR3:
    - API response time: Verify p95 < 500ms
    - Workflow execution: Verify p95 < 5 seconds
    - Document any violations
  - [ ] Identify performance issues:
    - Slow endpoints (p95 > 500ms)
    - Slow workflows (p95 > 5 seconds)
    - High error rates (> 1%)
  - [ ] Create performance report:
    - Document benchmark results
    - Document violations
    - Document recommendations

- [ ] **Task 6: Optimize database queries** (AC: 6)
  - [ ] Analyze database query performance:
    - Enable query logging in Supabase
    - Run load tests and collect slow queries
    - Identify queries with high execution time (> 100ms)
  - [ ] Identify frequently queried fields:
    - `user_id`: Used in most queries (filter by user)
    - `status`: Used in prospect/campaign filtering
    - `created_at`: Used in sorting and date filtering
  - [ ] **Verify existing indexes:**
    - [ ] Query existing indexes: `SELECT * FROM pg_indexes WHERE schemaname = 'public' AND tablename IN ('prospects', 'campaigns', 'meetings', 'ai_conversation_log')`
    - [ ] Document existing indexes:
      - `prospects`: `idx_prospects_user_id`, `idx_prospects_status`, `idx_prospects_created_at` (verify if exists)
      - `campaigns`: `idx_campaigns_user_id`, `idx_campaigns_status`, `idx_campaigns_created_at` (verify if exists)
      - `meetings`: `idx_meetings_user_id`, `idx_meetings_status`, `idx_meetings_scheduled_at` (verify if exists)
      - `ai_conversation_log`: `idx_conversation_user_id`, `idx_conversation_prospect_id`, `idx_conversation_created_at` (verify if exists)
    - [ ] Identify missing indexes (only create indexes that don't exist)
    - [ ] Check for composite indexes: `idx_prospects_user_status` (verify if exists)
  - [ ] Create database migration: `supabase/migrations/YYYYMMDDHHMMSS_add_performance_indexes.sql`
    - **Note:** Only create indexes that don't already exist (verified above)
    - Add indexes on `prospects` table (if missing):
      - `CREATE INDEX IF NOT EXISTS idx_prospects_user_id ON prospects(user_id)` (may already exist)
      - `CREATE INDEX IF NOT EXISTS idx_prospects_status ON prospects(status)` (may already exist)
      - `CREATE INDEX IF NOT EXISTS idx_prospects_created_at ON prospects(created_at)` (verify if exists)
      - `CREATE INDEX IF NOT EXISTS idx_prospects_user_status ON prospects(user_id, status)` (composite - verify if exists)
    - Add indexes on `campaigns` table (if missing):
      - `CREATE INDEX IF NOT EXISTS idx_campaigns_user_id ON campaigns(user_id)` (may already exist)
      - `CREATE INDEX IF NOT EXISTS idx_campaigns_status ON campaigns(status)` (may already exist)
      - `CREATE INDEX IF NOT EXISTS idx_campaigns_created_at ON campaigns(created_at)` (may already exist)
    - Add indexes on `meetings` table (if missing):
      - `CREATE INDEX IF NOT EXISTS idx_meetings_user_id ON meetings(user_id)` (may already exist)
      - `CREATE INDEX IF NOT EXISTS idx_meetings_status ON meetings(status)` (may already exist)
      - `CREATE INDEX IF NOT EXISTS idx_meetings_scheduled_at ON meetings(scheduled_at)` (may already exist)
    - Add indexes on `ai_conversation_log` table (if missing):
      - `CREATE INDEX IF NOT EXISTS idx_ai_conversation_log_user_id ON ai_conversation_log(user_id)` (may already exist as `idx_conversation_user_id`)
      - `CREATE INDEX IF NOT EXISTS idx_ai_conversation_log_prospect_id ON ai_conversation_log(prospect_id)` (may already exist as `idx_conversation_prospect_id`)
      - `CREATE INDEX IF NOT EXISTS idx_ai_conversation_log_created_at ON ai_conversation_log(created_at)` (may already exist as `idx_conversation_created_at`)
  - [ ] Verify indexes created:
    - Query: `SELECT * FROM pg_indexes WHERE tablename = 'prospects'`
    - Verify all indexes exist
  - [ ] Test query performance improvement:
    - Run queries before and after index creation
    - Compare execution times
    - Document performance improvement
  - [ ] Apply migration to database
  - [ ] Re-run load tests:
    - Compare performance before and after indexes
    - Document improvement

- [ ] **Task 7: Create load testing report** (AC: 7)
  - [ ] Compile test results:
    - Scenario 1 results (LinkedIn scraping)
    - Scenario 2 results (Email queue)
    - Scenario 3 results (AI enrichment)
    - Performance benchmark results
    - Database optimization results
  - [ ] Document bottlenecks:
    - N8N workflow capacity limits
    - Upstash Redis performance limits
    - Claude API rate limits
    - Database performance limits
    - API Gateway performance limits
  - [ ] Document scaling limits:
    - Maximum concurrent users supported
    - Maximum workflows per minute
    - Maximum email queue operations per second
    - Maximum AI enrichment requests per minute
  - [ ] Document recommended infrastructure upgrades:
    - N8N: Upgrade plan (if needed)
    - Upstash: Upgrade plan (if needed)
    - Supabase: Upgrade plan (if needed)
    - API Gateway: Scaling recommendations
  - [ ] Create report document: `docs/load-testing-report.md`
    - Executive summary
    - Test scenarios and results
    - Bottlenecks identified
    - Scaling limits
    - Infrastructure upgrade recommendations
    - Performance benchmarks validation
  - [ ] Present report to stakeholders:
    - Review findings
    - Discuss upgrade recommendations
    - Plan next steps

## Dev Notes

### Architecture Context

**Performance Requirements (NFR3):**
API response time < 500ms p95, workflow execution < 5 seconds. Performance benchmarks must be validated through load testing.
[Source: docs/prd/requirements.md#nfr3]

**Scalability Requirements (NFR6):**
System must handle 100 concurrent users to scale to 50+ paying customers. Load testing validates this requirement.
[Source: docs/prd/requirements.md#nfr6]

**N8N Capacity:**
Assumed capable of handling 100 concurrent users based on 22 existing workflow experience - requires load testing validation.
[Source: docs/prd/technical-assumptions.md#scalability-assumptions]

**Database Performance:**
PostgreSQL can handle 100K prospect records without partitioning - monitor query performance at 50K+ records. Add indexes on frequently queried fields.
[Source: docs/prd/technical-assumptions.md#scalability-assumptions]

**Load Testing Tools:**
k6 or Artillery recommended for load testing. k6 preferred for API-focused testing.
[Source: docs/prd/epic-6-production-readiness-scale-prep.md#story-63]

**Claude API Rate Limits:**
Monitor Claude API rate limits during load testing. Implement rate limiting handling if needed.
[Source: docs/prd/technical-assumptions.md#ai-cost-management]

**Project Structure:**
Load testing scripts: `tests/load/scenarios/*.js`. Load testing results: `tests/load/results/*.json`. Load testing report: `docs/load-testing-report.md`.
[Source: architecture/unified-project-structure.md]

**Database Indexes:**
Add indexes on frequently queried fields: `user_id`, `status`, `created_at`. Composite indexes for common query patterns.
[Source: docs/prd/epic-6-production-readiness-scale-prep.md#story-63]

### Testing

**Test File Location:**
- Load testing scripts: `tests/load/scenarios/*.js`
- Load testing results: `tests/load/results/*.json`
- Load testing report: `docs/load-testing-report.md`

**Testing Standards:**
- Run load tests in staging environment (not production)
- Use realistic test data (similar to production)
- Monitor all systems during tests (N8N, Supabase, Upstash, Claude API)
- Document all results and bottlenecks
- Validate performance benchmarks (NFR3)

**Testing Frameworks:**
- Load testing: k6 (JavaScript-based)
- Performance monitoring: Sentry, Betterstack (from Story 6.1)
- Database monitoring: Supabase dashboard, query logs

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| TBD | 1.0 | Initial story creation | Sarah (PO) |
| 2025-01-17 | 1.1 | Story corrections: Added index verification task, endpoint verification tasks, staging environment setup, test user management strategy, Claude API rate limits documentation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
TBD

### Debug Log References
TBD

### Completion Notes List
TBD

### File List
TBD

## QA Results
TBD

